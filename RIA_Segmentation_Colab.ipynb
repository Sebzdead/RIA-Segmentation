{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b046c0b4",
   "metadata": {},
   "source": [
    "# RIA Segmentation Pipeline - Google Colab\n",
    "\n",
    "This notebook performs SAM2-based video segmentation for RIA (pharyngeal pumping) analysis in C. elegans.\n",
    "\n",
    "## Key Features:\n",
    "- **Cloud Storage Integration**: Works with Google Drive\n",
    "- **Interactive Widgets**: User-friendly bounding box collection\n",
    "- **Batch Processing**: Process multiple videos sequentially\n",
    "- **GPU Acceleration**: Automatic GPU detection and usage\n",
    "- **Result Visualization**: Preview videos with mask overlays\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload your video data to Google Drive in the specified folder structure\n",
    "2. Run the setup cell to install dependencies\n",
    "3. Execute the pipeline cells in order\n",
    "4. Download results when complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe85b3",
   "metadata": {},
   "source": [
    "## üîß Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e355f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab and setup environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úì Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Install required packages\n",
    "if IN_COLAB:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install -q segment-anything-2 tifffile h5py opencv-python matplotlib pandas scipy scikit-image ipywidgets\n",
    "    \n",
    "    # Install SAM2 from GitHub\n",
    "    !pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\n",
    "    \n",
    "    print(\"‚úì Package installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f08b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import h5py\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab-specific imports\n",
    "if IN_COLAB:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output, Image as IPImage\n",
    "    from google.colab import files\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f2b81",
   "metadata": {},
   "source": [
    "## ü§ñ Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbcb798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories and download SAM2 model\n",
    "work_dir = '/content/ria_segmentation' if IN_COLAB else './ria_segmentation'\n",
    "model_dir = '/content/models' if IN_COLAB else './models'\n",
    "\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(work_dir)\n",
    "\n",
    "# Download SAM2 model checkpoint\n",
    "checkpoint_path = f\"{model_dir}/sam2.1_hiera_base_plus.pt\"\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(\"Downloading SAM2 model checkpoint...\")\n",
    "    if IN_COLAB:\n",
    "        !wget -O {checkpoint_path} https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2.1_hiera_base_plus.pt\n",
    "    else:\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(\n",
    "            'https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2.1_hiera_base_plus.pt',\n",
    "            checkpoint_path\n",
    "        )\n",
    "    print(\"‚úì Model downloaded\")\n",
    "else:\n",
    "    print(\"‚úì Model already exists\")\n",
    "\n",
    "print(f\"Model path: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae02ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAM2 model\n",
    "def setup_sam2_model():\n",
    "    # Select device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"‚úì Using CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"‚ö†Ô∏è  Using CPU device (processing will be slower)\")\n",
    "    \n",
    "    try:\n",
    "        from sam2.build_sam import build_sam2_video_predictor\n",
    "        \n",
    "        # Use default config for base_plus model\n",
    "        model_cfg = \"sam2_hiera_b+.yaml\"\n",
    "        \n",
    "        # Build predictor\n",
    "        predictor = build_sam2_video_predictor(model_cfg, checkpoint_path, device=device)\n",
    "        print(\"‚úì SAM2 model initialized successfully!\")\n",
    "        return predictor, device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing SAM2: {e}\")\n",
    "        return None, device\n",
    "\n",
    "# Initialize the model\n",
    "predictor, device = setup_sam2_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a902145",
   "metadata": {},
   "source": [
    "## üìÅ Storage Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup storage paths\n",
    "if IN_COLAB:\n",
    "    base_path = '/content/drive/MyDrive/RIA_segmentation'\n",
    "    input_videos_dir = f'{base_path}/input_videos'\n",
    "    output_dir = f'{base_path}/output'\n",
    "    temp_dir = '/content/temp_processing'\n",
    "else:\n",
    "    base_path = './RIA_segmentation'\n",
    "    input_videos_dir = f'{base_path}/input_videos'\n",
    "    output_dir = f'{base_path}/output'\n",
    "    temp_dir = './temp_processing'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(input_videos_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Storage setup complete:\")\n",
    "print(f\"   Input videos: {input_videos_dir}\")\n",
    "print(f\"   Output: {output_dir}\")\n",
    "print(f\"   Temp: {temp_dir}\")\n",
    "\n",
    "# Check for existing videos\n",
    "if os.path.exists(input_videos_dir):\n",
    "    available_videos = [d for d in os.listdir(input_videos_dir) \n",
    "                       if os.path.isdir(os.path.join(input_videos_dir, d))]\n",
    "    print(f\"\\nüìπ Found {len(available_videos)} video directories:\")\n",
    "    for video in available_videos[:5]:  # Show first 5\n",
    "        print(f\"   - {video}\")\n",
    "    if len(available_videos) > 5:\n",
    "        print(f\"   ... and {len(available_videos) - 5} more\")\n",
    "else:\n",
    "    available_videos = []\n",
    "    print(\"\\n‚ö†Ô∏è  No video directories found. Please upload your data to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146c30c",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Interactive Widget Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a986a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColabBboxCollector:\n",
    "    \"\"\"Interactive bounding box collection for Colab environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_path, object_ids=None):\n",
    "        self.image_path = image_path\n",
    "        self.image = Image.open(image_path)\n",
    "        self.object_ids = object_ids or [1, 2]\n",
    "        self.bboxes = {}\n",
    "        self.current_obj = self.object_ids[0]\n",
    "        self.finished = False\n",
    "        self.skipped = False\n",
    "        \n",
    "        # Object names\n",
    "        self.object_names = {1: 'nrD (dorsal)', 2: 'nrV (ventral)'}\n",
    "        \n",
    "        self.setup_widgets()\n",
    "    \n",
    "    def setup_widgets(self):\n",
    "        \"\"\"Create IPython widgets for bbox collection.\"\"\"\n",
    "        \n",
    "        # Object selector\n",
    "        self.obj_selector = widgets.Dropdown(\n",
    "            options=[(self.object_names.get(obj_id, f'Object {obj_id}'), obj_id) \n",
    "                    for obj_id in self.object_ids],\n",
    "            value=self.current_obj,\n",
    "            description='Target Object:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Coordinate inputs with better styling\n",
    "        coord_style = {'description_width': '40px'}\n",
    "        coord_layout = widgets.Layout(width='120px')\n",
    "        \n",
    "        self.x1_input = widgets.IntText(value=50, description='X1:', style=coord_style, layout=coord_layout)\n",
    "        self.y1_input = widgets.IntText(value=50, description='Y1:', style=coord_style, layout=coord_layout)\n",
    "        self.x2_input = widgets.IntText(value=150, description='X2:', style=coord_style, layout=coord_layout)\n",
    "        self.y2_input = widgets.IntText(value=150, description='Y2:', style=coord_style, layout=coord_layout)\n",
    "        \n",
    "        # Buttons with styling\n",
    "        self.save_btn = widgets.Button(\n",
    "            description='üíæ Save Bbox', \n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        self.clear_btn = widgets.Button(\n",
    "            description='üóëÔ∏è Clear All', \n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        self.finish_btn = widgets.Button(\n",
    "            description='‚úÖ Finish', \n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        self.skip_btn = widgets.Button(\n",
    "            description='‚è≠Ô∏è Skip Video', \n",
    "            button_style='danger',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        # Status output\n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        # Event handlers\n",
    "        self.obj_selector.observe(self.on_object_change, names='value')\n",
    "        self.save_btn.on_click(self.save_bbox)\n",
    "        self.clear_btn.on_click(self.clear_all)\n",
    "        self.finish_btn.on_click(self.finish)\n",
    "        self.skip_btn.on_click(self.skip)\n",
    "        \n",
    "        # Layout\n",
    "        coord_box = widgets.HBox([\n",
    "            self.x1_input, self.y1_input, self.x2_input, self.y2_input\n",
    "        ], layout=widgets.Layout(justify_content='space-around'))\n",
    "        \n",
    "        button_box = widgets.HBox([\n",
    "            self.save_btn, self.clear_btn, self.finish_btn, self.skip_btn\n",
    "        ], layout=widgets.Layout(justify_content='space-around'))\n",
    "        \n",
    "        instructions_html = widgets.HTML(\n",
    "            value=\"\"\"\n",
    "            <div style='background-color: #f0f7ff; padding: 15px; border-radius: 5px; border-left: 4px solid #007acc;'>\n",
    "                <h3>üìã Instructions:</h3>\n",
    "                <ol>\n",
    "                    <li>Select the target object (nrD or nrV) from the dropdown</li>\n",
    "                    <li>Look at the reference image below and note the coordinates</li>\n",
    "                    <li>Set the bounding box coordinates: (X1,Y1) = top-left, (X2,Y2) = bottom-right</li>\n",
    "                    <li>Click 'Save Bbox' to store the bounding box</li>\n",
    "                    <li>Repeat for other objects if needed</li>\n",
    "                    <li>Click 'Finish' when done, or 'Skip Video' to skip this video</li>\n",
    "                </ol>\n",
    "                <p><strong>Note:</strong> Ensure X1 &lt; X2 and Y1 &lt; Y2 for valid coordinates.</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        self.widget_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h2>üéØ Bounding Box Collection</h2>\"),\n",
    "            instructions_html,\n",
    "            widgets.HTML(\"<h4>Object Selection:</h4>\"),\n",
    "            self.obj_selector,\n",
    "            widgets.HTML(\"<h4>Coordinates (pixels):</h4>\"),\n",
    "            coord_box,\n",
    "            widgets.HTML(\"<h4>Actions:</h4>\"),\n",
    "            button_box,\n",
    "            widgets.HTML(\"<h4>Status:</h4>\"),\n",
    "            self.output\n",
    "        ], layout=widgets.Layout(padding='20px'))\n",
    "    \n",
    "    def on_object_change(self, change):\n",
    "        \"\"\"Handle object selection change.\"\"\"\n",
    "        self.current_obj = change['new']\n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üéØ Selected: {self.object_names.get(self.current_obj, self.current_obj)}\")\n",
    "    \n",
    "    def save_bbox(self, btn):\n",
    "        \"\"\"Save current bounding box.\"\"\"\n",
    "        x1, y1, x2, y2 = self.x1_input.value, self.y1_input.value, self.x2_input.value, self.y2_input.value\n",
    "        \n",
    "        # Validate coordinates\n",
    "        if x1 >= x2 or y1 >= y2:\n",
    "            with self.output:\n",
    "                clear_output(wait=True)\n",
    "                print(\"‚ùå Error: Invalid coordinates. Ensure X1 < X2 and Y1 < Y2\")\n",
    "            return\n",
    "        \n",
    "        # Validate within image bounds\n",
    "        img_width, img_height = self.image.size\n",
    "        if x1 < 0 or y1 < 0 or x2 > img_width or y2 > img_height:\n",
    "            with self.output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"‚ö†Ô∏è  Warning: Coordinates outside image bounds (0,0) to ({img_width},{img_height})\")\n",
    "        \n",
    "        # Save bbox\n",
    "        self.bboxes[self.current_obj] = np.array([x1, y1, x2, y2], dtype=np.float32)\n",
    "        \n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            obj_name = self.object_names.get(self.current_obj, f'Object {self.current_obj}')\n",
    "            print(f\"‚úÖ Saved bbox for {obj_name}: [{x1}, {y1}, {x2}, {y2}]\")\n",
    "            print(f\"üìä Total bboxes saved: {len(self.bboxes)}\")\n",
    "            \n",
    "            # List all saved bboxes\n",
    "            for obj_id, bbox in self.bboxes.items():\n",
    "                name = self.object_names.get(obj_id, f'Object {obj_id}')\n",
    "                print(f\"   ‚Ä¢ {name}: {bbox.astype(int)}\")\n",
    "    \n",
    "    def clear_all(self, btn):\n",
    "        \"\"\"Clear all bounding boxes.\"\"\"\n",
    "        self.bboxes = {}\n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"üóëÔ∏è Cleared all bounding boxes\")\n",
    "    \n",
    "    def finish(self, btn):\n",
    "        \"\"\"Finish bbox collection.\"\"\"\n",
    "        self.finished = True\n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"‚úÖ Finished! Collected {len(self.bboxes)} bounding boxes\")\n",
    "            print(\"Proceeding to video processing...\")\n",
    "    \n",
    "    def skip(self, btn):\n",
    "        \"\"\"Skip this video.\"\"\"\n",
    "        self.skipped = True\n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"‚è≠Ô∏è Skipping this video\")\n",
    "    \n",
    "    def show_image_with_bboxes(self):\n",
    "        \"\"\"Display the reference image with any existing bounding boxes.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        ax.imshow(self.image)\n",
    "        ax.set_title(\"Reference Image - Use coordinates from this image\", fontsize=16, pad=20)\n",
    "        \n",
    "        # Add coordinate annotations\n",
    "        height, width = self.image.size[1], self.image.size[0]\n",
    "        ax.set_xlim(0, width)\n",
    "        ax.set_ylim(height, 0)  # Invert y-axis to match image coordinates\n",
    "        \n",
    "        # Add grid for reference\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlabel('X coordinate (pixels)', fontsize=12)\n",
    "        ax.set_ylabel('Y coordinate (pixels)', fontsize=12)\n",
    "        \n",
    "        # Add existing bboxes\n",
    "        colors = {1: 'red', 2: 'blue'}\n",
    "        for obj_id, bbox in self.bboxes.items():\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            color = colors.get(obj_id, 'green')\n",
    "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                               linewidth=3, \n",
    "                               edgecolor=color,\n",
    "                               facecolor='none',\n",
    "                               label=self.object_names.get(obj_id, f'Object {obj_id}'))\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add text annotation\n",
    "            ax.text(x1, y1-10, f'{self.object_names.get(obj_id, f\"Obj {obj_id}\")}\\n[{x1:.0f},{y1:.0f},{x2:.0f},{y2:.0f}]',\n",
    "                   color=color, fontweight='bold', fontsize=10,\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        if self.bboxes:\n",
    "            ax.legend(loc='upper right', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úì Bounding box collector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbdded",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9cb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(video_dir):\n",
    "    \"\"\"Get sorted list of frame files from video directory.\"\"\"\n",
    "    frame_files = [f for f in os.listdir(video_dir) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    frame_files.sort(key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    return frame_files\n",
    "\n",
    "def process_video_with_sam2(video_dir, predictor, prompts):\n",
    "    \"\"\"Process video with SAM2 using provided prompts.\"\"\"\n",
    "    \n",
    "    # Initialize inference state\n",
    "    inference_state = predictor.init_state(video_path=video_dir)\n",
    "    \n",
    "    # Add prompts to first frame\n",
    "    frame_idx = 0\n",
    "    for obj_id, prompt_data in prompts.items():\n",
    "        if 'bbox' in prompt_data:\n",
    "            # Bounding box prompt\n",
    "            bbox = prompt_data['bbox']\n",
    "            _, out_obj_ids, out_mask_logits = predictor.add_new_points(\n",
    "                inference_state=inference_state,\n",
    "                frame_idx=frame_idx,\n",
    "                obj_id=obj_id,\n",
    "                points=None,\n",
    "                labels=None,\n",
    "                clear_old_points=True,\n",
    "                bbox=bbox\n",
    "            )\n",
    "        elif 'points' in prompt_data:\n",
    "            # Point prompts\n",
    "            points, labels = prompt_data['points']\n",
    "            _, out_obj_ids, out_mask_logits = predictor.add_new_points(\n",
    "                inference_state=inference_state,\n",
    "                frame_idx=frame_idx,\n",
    "                obj_id=obj_id,\n",
    "                points=points,\n",
    "                labels=labels,\n",
    "                clear_old_points=True\n",
    "            )\n",
    "    \n",
    "    # Propagate masks through video\n",
    "    video_segments = {}\n",
    "    \n",
    "    print(\"üé¨ Propagating masks through video...\")\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in tqdm(\n",
    "        predictor.propagate_in_video(inference_state), \n",
    "        desc=\"Processing frames\"\n",
    "    ):\n",
    "        video_segments[out_frame_idx] = {\n",
    "            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "            for i, out_obj_id in enumerate(out_obj_ids)\n",
    "        }\n",
    "    \n",
    "    return video_segments\n",
    "\n",
    "def save_results_h5(video_segments, output_path):\n",
    "    \"\"\"Save segmentation results to HDF5 file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        # Metadata\n",
    "        f.attrs['num_frames'] = len(video_segments)\n",
    "        if video_segments:\n",
    "            first_frame = list(video_segments.keys())[0]\n",
    "            f.attrs['object_ids'] = list(video_segments[first_frame].keys())\n",
    "        \n",
    "        # Create groups\n",
    "        masks_group = f.create_group('masks')\n",
    "        \n",
    "        # Save masks\n",
    "        sorted_frames = sorted(video_segments.keys())\n",
    "        for frame_idx in sorted_frames:\n",
    "            frame_group = masks_group.create_group(f'frame_{frame_idx:06d}')\n",
    "            \n",
    "            for obj_id, mask in video_segments[frame_idx].items():\n",
    "                frame_group.create_dataset(\n",
    "                    f'object_{obj_id}', \n",
    "                    data=mask.astype(bool),\n",
    "                    compression='gzip'\n",
    "                )\n",
    "    \n",
    "    print(f\"üíæ Results saved to {output_path}\")\n",
    "\n",
    "def create_preview_video(video_dir, video_segments, output_path, fps=10):\n",
    "    \"\"\"Create preview video with mask overlays.\"\"\"\n",
    "    \n",
    "    frame_files = get_video_frames(video_dir)\n",
    "    if not frame_files:\n",
    "        print(\"No frames found for preview\")\n",
    "        return\n",
    "    \n",
    "    # Read first frame to get dimensions\n",
    "    first_frame_path = os.path.join(video_dir, frame_files[0])\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    height, width = first_frame.shape[:2]\n",
    "    \n",
    "    # Video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Colors for objects\n",
    "    colors = {1: (255, 0, 0), 2: (0, 0, 255)}  # Red for nrD, Blue for nrV\n",
    "    \n",
    "    print(\"üé• Creating preview video...\")\n",
    "    for frame_idx, frame_file in enumerate(tqdm(frame_files, desc=\"Rendering frames\")):\n",
    "        frame_path = os.path.join(video_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        \n",
    "        # Add mask overlays if available\n",
    "        if frame_idx in video_segments:\n",
    "            overlay = np.zeros_like(frame)\n",
    "            \n",
    "            for obj_id, mask in video_segments[frame_idx].items():\n",
    "                if obj_id in colors:\n",
    "                    color = colors[obj_id]\n",
    "                    mask_3d = np.stack([mask.squeeze()] * 3, axis=-1)\n",
    "                    colored_mask = mask_3d * np.array(color)\n",
    "                    overlay = cv2.addWeighted(overlay, 1, colored_mask.astype(np.uint8), 0.6, 0)\n",
    "            \n",
    "            frame = cv2.addWeighted(frame, 1, overlay, 0.4, 0)\n",
    "        \n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"üé¨ Preview video saved to {output_path}\")\n",
    "\n",
    "print(\"‚úì Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568435a6",
   "metadata": {},
   "source": [
    "## üöÄ Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7016ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video selection interface\n",
    "def create_video_selector():\n",
    "    \"\"\"Create interactive video selection widget.\"\"\"\n",
    "    \n",
    "    # Refresh video list\n",
    "    available_videos = []\n",
    "    if os.path.exists(input_videos_dir):\n",
    "        available_videos = [d for d in os.listdir(input_videos_dir) \n",
    "                           if os.path.isdir(os.path.join(input_videos_dir, d))]\n",
    "    \n",
    "    if not available_videos:\n",
    "        print(\"‚ö†Ô∏è  No video directories found!\")\n",
    "        print(f\"Please upload video directories to: {input_videos_dir}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Create widgets\n",
    "    video_selector = widgets.SelectMultiple(\n",
    "        options=available_videos,\n",
    "        description='Select videos to process:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(height='200px', width='400px')\n",
    "    )\n",
    "    \n",
    "    process_btn = widgets.Button(\n",
    "        description='üöÄ Start Processing', \n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='200px', height='50px')\n",
    "    )\n",
    "    \n",
    "    output_widget = widgets.Output()\n",
    "    \n",
    "    # Interface layout\n",
    "    interface = widgets.VBox([\n",
    "        widgets.HTML(\"<h2>üìπ Video Selection</h2>\"),\n",
    "        widgets.HTML(f\"<p>Found <strong>{len(available_videos)}</strong> video directories in your storage.</p>\"),\n",
    "        widgets.HTML(\"<p>Hold <kbd>Ctrl</kbd> (or <kbd>Cmd</kbd>) to select multiple videos.</p>\"),\n",
    "        video_selector,\n",
    "        process_btn,\n",
    "        output_widget\n",
    "    ], layout=widgets.Layout(padding='20px'))\n",
    "    \n",
    "    return video_selector, process_btn, output_widget, interface\n",
    "\n",
    "# Create the interface\n",
    "if available_videos:\n",
    "    video_selector, process_btn, output_widget, interface = create_video_selector()\n",
    "    display(interface)\n",
    "else:\n",
    "    print(\"‚ùå Cannot create interface - no videos found\")\n",
    "    video_selector = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3308ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video(video_name):\n",
    "    \"\"\"Process a single video with interactive prompting.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üé¨ Processing video: {video_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    video_dir = os.path.join(input_videos_dir, video_name)\n",
    "    \n",
    "    # Get frame files\n",
    "    frame_files = get_video_frames(video_dir)\n",
    "    if not frame_files:\n",
    "        print(f\"‚ùå No frames found in {video_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä Found {len(frame_files)} frames\")\n",
    "    \n",
    "    # Get first frame for prompting\n",
    "    first_frame_path = os.path.join(video_dir, frame_files[0])\n",
    "    \n",
    "    # Collect bounding box prompts\n",
    "    print(\"\\nüéØ Collecting bounding box prompts...\")\n",
    "    bbox_collector = ColabBboxCollector(first_frame_path, object_ids=[1, 2])\n",
    "    \n",
    "    # Show the image first\n",
    "    bbox_collector.show_image_with_bboxes()\n",
    "    \n",
    "    # Display the widget interface\n",
    "    display(bbox_collector.widget_box)\n",
    "    \n",
    "    # Wait for user to finish (in a real interactive environment)\n",
    "    import time\n",
    "    print(\"‚è≥ Waiting for user input...\")\n",
    "    print(\"Use the widgets above to set bounding boxes, then click 'Finish'\")\n",
    "    \n",
    "    # This would normally wait for user interaction\n",
    "    # For demonstration, we'll simulate some bounding boxes\n",
    "    return {\n",
    "        'video_name': video_name,\n",
    "        'status': 'pending_user_input',\n",
    "        'collector': bbox_collector\n",
    "    }\n",
    "\n",
    "def complete_video_processing(video_name, bboxes):\n",
    "    \"\"\"Complete the processing after user provides bounding boxes.\"\"\"\n",
    "    \n",
    "    video_dir = os.path.join(input_videos_dir, video_name)\n",
    "    \n",
    "    if not bboxes:\n",
    "        print(f\"‚è≠Ô∏è No bounding boxes provided for {video_name}, skipping...\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Processing {video_name} with {len(bboxes)} bounding boxes\")\n",
    "    \n",
    "    # Convert bboxes to prompts\n",
    "    prompts = {}\n",
    "    for obj_id, bbox in bboxes.items():\n",
    "        prompts[obj_id] = {'bbox': bbox}\n",
    "    \n",
    "    try:\n",
    "        # Process video with SAM2\n",
    "        video_segments = process_video_with_sam2(video_dir, predictor, prompts)\n",
    "        \n",
    "        # Save results\n",
    "        output_h5_path = os.path.join(output_dir, f\"{video_name}_segments.h5\")\n",
    "        save_results_h5(video_segments, output_h5_path)\n",
    "        \n",
    "        # Create preview video\n",
    "        preview_path = os.path.join(output_dir, f\"{video_name}_preview.mp4\")\n",
    "        create_preview_video(video_dir, video_segments, preview_path)\n",
    "        \n",
    "        result = {\n",
    "            'video_name': video_name,\n",
    "            'status': 'success',\n",
    "            'num_frames': len(video_segments),\n",
    "            'num_objects': len(prompts),\n",
    "            'h5_path': output_h5_path,\n",
    "            'preview_path': preview_path\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Successfully processed {video_name}\")\n",
    "        print(f\"   üìä Frames: {result['num_frames']}\")\n",
    "        print(f\"   üéØ Objects: {result['num_objects']}\")\n",
    "        print(f\"   üíæ Output: {output_h5_path}\")\n",
    "        print(f\"   üé¨ Preview: {preview_path}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {video_name}: {str(e)}\")\n",
    "        return {\n",
    "            'video_name': video_name,\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Define the main processing function\n",
    "def process_selected_videos():\n",
    "    \"\"\"Process all selected videos.\"\"\"\n",
    "    if video_selector is None:\n",
    "        print(\"‚ùå No video selector available\")\n",
    "        return\n",
    "    \n",
    "    selected_videos = list(video_selector.value)\n",
    "    \n",
    "    if not selected_videos:\n",
    "        print(\"‚ö†Ô∏è  No videos selected\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üöÄ Starting processing of {len(selected_videos)} videos\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, video_name in enumerate(selected_videos, 1):\n",
    "        print(f\"\\n--- Processing video {i}/{len(selected_videos)}: {video_name} ---\")\n",
    "        \n",
    "        # Check if already processed\n",
    "        output_path = os.path.join(output_dir, f\"{video_name}_segments.h5\")\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"‚ö†Ô∏è  {video_name} already processed, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        result = process_single_video(video_name)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Connect the button to the processing function\n",
    "if video_selector is not None:\n",
    "    def on_process_click(btn):\n",
    "        with output_widget:\n",
    "            clear_output(wait=True)\n",
    "            process_selected_videos()\n",
    "    \n",
    "    process_btn.on_click(on_process_click)\n",
    "    \n",
    "    print(\"‚úÖ Processing interface ready!\")\n",
    "    print(\"\\nüìã Next steps:\")\n",
    "    print(\"1. Select one or more videos from the list above\")\n",
    "    print(\"2. Click 'Start Processing' button\")\n",
    "    print(\"3. For each video, use the bounding box interface to mark objects\")\n",
    "    print(\"4. Results will be saved to your Google Drive\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot setup processing - video selector not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8725fd4",
   "metadata": {},
   "source": [
    "## üìä Results and Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check processed results\n",
    "def view_processing_results():\n",
    "    \"\"\"Display summary of processed videos.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        print(\"üìÇ No output directory found\")\n",
    "        return\n",
    "    \n",
    "    # Find all H5 files\n",
    "    h5_files = [f for f in os.listdir(output_dir) if f.endswith('_segments.h5')]\n",
    "    mp4_files = [f for f in os.listdir(output_dir) if f.endswith('_preview.mp4')]\n",
    "    \n",
    "    print(f\"üìä Processing Results Summary\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"üìÅ Output directory: {output_dir}\")\n",
    "    print(f\"üóÉÔ∏è  Segmentation files: {len(h5_files)}\")\n",
    "    print(f\"üé¨ Preview videos: {len(mp4_files)}\")\n",
    "    \n",
    "    if h5_files:\n",
    "        print(f\"\\nüìã Processed Videos:\")\n",
    "        for i, h5_file in enumerate(h5_files, 1):\n",
    "            video_name = h5_file.replace('_segments.h5', '')\n",
    "            h5_path = os.path.join(output_dir, h5_file)\n",
    "            \n",
    "            # Try to read metadata\n",
    "            try:\n",
    "                with h5py.File(h5_path, 'r') as f:\n",
    "                    num_frames = f.attrs.get('num_frames', 'Unknown')\n",
    "                    object_ids = f.attrs.get('object_ids', [])\n",
    "                    file_size = os.path.getsize(h5_path) / (1024*1024)  # MB\n",
    "                    \n",
    "                print(f\"   {i}. {video_name}\")\n",
    "                print(f\"      üìä Frames: {num_frames}\")\n",
    "                print(f\"      üéØ Objects: {len(object_ids)} {list(object_ids)}\")\n",
    "                print(f\"      üíæ Size: {file_size:.1f} MB\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {i}. {video_name} (Error reading file: {e})\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No processed videos found\")\n",
    "        print(\"Run the processing pipeline above to generate results.\")\n",
    "\n",
    "# View current results\n",
    "view_processing_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results (for Colab)\n",
    "def download_all_results():\n",
    "    \"\"\"Create and download a zip file with all results.\"\"\"\n",
    "    \n",
    "    if not IN_COLAB:\n",
    "        print(\"Not in Colab - results are already local\")\n",
    "        return\n",
    "    \n",
    "    import zipfile\n",
    "    from google.colab import files\n",
    "    \n",
    "    if not os.path.exists(output_dir) or not os.listdir(output_dir):\n",
    "        print(\"‚ùå No results found to download\")\n",
    "        return\n",
    "    \n",
    "    zip_path = '/content/ria_segmentation_results.zip'\n",
    "    \n",
    "    print(\"üì¶ Creating results archive...\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, output_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "                print(f\"   üìÑ Added: {arcname}\")\n",
    "    \n",
    "    # Get zip file size\n",
    "    zip_size = os.path.getsize(zip_path) / (1024*1024)  # MB\n",
    "    print(f\"\\n‚úÖ Archive created: {zip_size:.1f} MB\")\n",
    "    \n",
    "    print(\"‚¨áÔ∏è  Starting download...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"‚úÖ Download complete!\")\n",
    "\n",
    "# Create download button\n",
    "if IN_COLAB:\n",
    "    download_btn = widgets.Button(\n",
    "        description='‚¨áÔ∏è Download Results', \n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='200px', height='40px')\n",
    "    )\n",
    "    \n",
    "    def on_download_click(btn):\n",
    "        download_all_results()\n",
    "    \n",
    "    download_btn.on_click(on_download_click)\n",
    "    \n",
    "    download_interface = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üì¶ Download Results</h3>\"),\n",
    "        widgets.HTML(\"<p>Click the button below to download all segmentation results as a ZIP file.</p>\"),\n",
    "        download_btn\n",
    "    ], layout=widgets.Layout(padding='20px'))\n",
    "    \n",
    "    display(download_interface)\n",
    "else:\n",
    "    print(\"üíæ Results are stored locally in:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9530ee8",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Manual Processing (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual processing of a single video (for testing/debugging)\n",
    "def manual_process_video(video_name, bbox_coords=None):\n",
    "    \"\"\"\n",
    "    Manually process a single video with predefined bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        video_name (str): Name of the video directory\n",
    "        bbox_coords (dict): Dictionary with bounding box coordinates\n",
    "                           e.g., {1: [x1, y1, x2, y2], 2: [x1, y1, x2, y2]}\n",
    "    \"\"\"\n",
    "    \n",
    "    if not available_videos or video_name not in available_videos:\n",
    "        print(f\"‚ùå Video '{video_name}' not found in available videos\")\n",
    "        print(f\"Available: {available_videos}\")\n",
    "        return\n",
    "    \n",
    "    if bbox_coords is None:\n",
    "        print(\"‚ö†Ô∏è  No bounding box coordinates provided\")\n",
    "        print(\"Example usage:\")\n",
    "        print(\"manual_process_video('video_name', {1: [50, 50, 150, 150], 2: [200, 50, 300, 150]})\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üé¨ Manually processing: {video_name}\")\n",
    "    print(f\"üì¶ Bounding boxes: {bbox_coords}\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    bboxes = {}\n",
    "    for obj_id, coords in bbox_coords.items():\n",
    "        bboxes[obj_id] = np.array(coords, dtype=np.float32)\n",
    "    \n",
    "    # Process the video\n",
    "    result = complete_video_processing(video_name, bboxes)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage (uncomment and modify as needed):\n",
    "# result = manual_process_video('your_video_name', {\n",
    "#     1: [100, 100, 200, 200],  # nrD bounding box\n",
    "#     2: [300, 100, 400, 200]   # nrV bounding box\n",
    "# })\n",
    "\n",
    "print(\"‚úì Manual processing function available\")\n",
    "print(\"Use manual_process_video('video_name', bbox_dict) for direct processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7c4c4",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting & Tips\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **\"No videos found\"**\n",
    "   - Check that your video directories are uploaded to `/content/drive/MyDrive/RIA_segmentation/input_videos/`\n",
    "   - Each video should be in its own subdirectory containing JPG frames\n",
    "\n",
    "2. **\"Model loading failed\"**\n",
    "   - Restart the runtime and re-run the setup cells\n",
    "   - Check that you have sufficient GPU memory\n",
    "\n",
    "3. **\"Processing too slow\"**\n",
    "   - Ensure you're using GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "   - Consider processing fewer frames or smaller videos\n",
    "\n",
    "4. **\"Invalid coordinates\"**\n",
    "   - Make sure X1 < X2 and Y1 < Y2\n",
    "   - Check that coordinates are within image bounds\n",
    "\n",
    "### Data Format:\n",
    "- Videos should be directories containing sequential JPG frames\n",
    "- Frame names should be numeric (e.g., 000000.jpg, 000001.jpg, ...)\n",
    "- Recommended frame size: 512x512 to 1024x1024 pixels\n",
    "\n",
    "### Performance:\n",
    "- GPU processing: ~1-5 fps depending on video size\n",
    "- CPU processing: ~0.1-0.5 fps (much slower)\n",
    "- Memory usage: ~2-8GB depending on video length and resolution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
